{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: Setup Kaggle API for Google Colab\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß SETTING UP KAGGLE API FOR GOOGLE COLAB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Upload kaggle.json\n",
    "from google.colab import files\n",
    "\n",
    "kaggle_path = Path.home() / '.kaggle'\n",
    "kaggle_path.mkdir(exist_ok=True)\n",
    "kaggle_json = kaggle_path / 'kaggle.json'\n",
    "\n",
    "if not kaggle_json.exists():\n",
    "    print(\"\\nüì§ Please upload your kaggle.json file:\")\n",
    "    print(\"   (Get it from: https://www.kaggle.com/settings ‚Üí Create New Token)\\n\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Move to correct location\n",
    "    for fn in uploaded.keys():\n",
    "        with open(kaggle_json, 'wb') as f:\n",
    "            f.write(uploaded[fn])\n",
    "    \n",
    "    # Set permissions\n",
    "    os.chmod(kaggle_json, 0o600)\n",
    "    print(\"‚úÖ Kaggle API configured!\")\n",
    "else:\n",
    "    print(\"‚úÖ Kaggle API already configured!\")\n",
    "\n",
    "# Install kaggle\n",
    "!pip install -q kaggle\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: Download Datasets from Kaggle\n",
    "# ============================================\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create data directory on Colab's fast local storage\n",
    "DATA_DIR = '/content/datasets'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# VERIFIED Kaggle dataset paths (tested and confirmed to exist)\n",
    "datasets = {\n",
    "    'rice': 'minhhuy2810/rice-diseases-image-dataset',\n",
    "    'cotton': 'janmejaybhoi/cotton-disease-dataset',  # Fixed: correct dataset\n",
    "    'wheat': 'olyadgetch/wheat-leaf-dataset', \n",
    "    'mango': 'aryashah2k/mango-leaf-disease-dataset',\n",
    "    'plantvillage': 'abdallahalidev/plantvillage-dataset'\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì• DOWNLOADING DATASETS FROM KAGGLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "failed_downloads = []\n",
    "\n",
    "for crop, dataset_name in datasets.items():\n",
    "    crop_dir = os.path.join(DATA_DIR, crop)\n",
    "    \n",
    "    if os.path.exists(crop_dir) and len(os.listdir(crop_dir)) > 0:\n",
    "        print(f\"‚úÖ {crop.upper():12s} - Already downloaded\")\n",
    "    else:\n",
    "        print(f\"\\n‚¨áÔ∏è  Downloading {crop.upper()} ({dataset_name})...\")\n",
    "        os.makedirs(crop_dir, exist_ok=True)\n",
    "        \n",
    "        # Run download with error handling\n",
    "        result = subprocess.run(\n",
    "            f\"kaggle datasets download -d {dataset_name} -p {crop_dir} --unzip\",\n",
    "            shell=True, capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {crop.upper():12s} - Downloaded!\")\n",
    "        else:\n",
    "            print(f\"‚ùå {crop.upper():12s} - FAILED: {result.stderr[:100]}\")\n",
    "            failed_downloads.append(crop)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä Checking downloaded data...\")\n",
    "\n",
    "total_files = 0\n",
    "for crop in datasets.keys():\n",
    "    crop_dir = os.path.join(DATA_DIR, crop)\n",
    "    if os.path.exists(crop_dir):\n",
    "        file_count = sum([len(files) for _, _, files in os.walk(crop_dir)])\n",
    "        total_files += file_count\n",
    "        status = \"‚úÖ\" if file_count > 0 else \"‚ùå\"\n",
    "        print(f\"  {status} {crop.upper():12s}: {file_count:,} files\")\n",
    "\n",
    "print(f\"\\nüìà Total files downloaded: {total_files:,}\")\n",
    "\n",
    "if failed_downloads:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed downloads: {', '.join(failed_downloads)}\")\n",
    "    print(\"   Try running this cell again or check dataset names on Kaggle\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: Install Packages & Setup\n",
    "# ============================================\n",
    "!pip install timm -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "\n",
    "# ‚ö° Speed optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Setup complete!\")\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: Load and Analyze Datasets\n",
    "# ============================================\n",
    "data_paths = {\n",
    "    'rice': os.path.join(DATA_DIR, 'rice'),\n",
    "    'cotton': os.path.join(DATA_DIR, 'cotton'),\n",
    "    'wheat': os.path.join(DATA_DIR, 'wheat'),\n",
    "    'mango': os.path.join(DATA_DIR, 'mango'),\n",
    "    'plantvillage': os.path.join(DATA_DIR, 'plantvillage')\n",
    "}\n",
    "\n",
    "def find_image_classes(base_path, max_depth=5):\n",
    "    \"\"\"Find all image class directories\"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG'}\n",
    "    class_data = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        depth = root[len(base_path):].count(os.sep)\n",
    "        if depth > max_depth:\n",
    "            continue\n",
    "            \n",
    "        image_files = [f for f in files if Path(f).suffix in image_extensions]\n",
    "        if image_files and len(image_files) > 50:\n",
    "            class_name = Path(root).name\n",
    "            if class_name not in class_data:\n",
    "                class_data[class_name] = []\n",
    "            class_data[class_name].extend([os.path.join(root, f) for f in image_files])\n",
    "    \n",
    "    return class_data\n",
    "\n",
    "# Analyze all datasets\n",
    "all_classes = {}\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ANALYZING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for crop, path in data_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\nüîç Analyzing {crop.upper()}...\")\n",
    "        classes = find_image_classes(path)\n",
    "        all_classes[crop] = classes\n",
    "        print(f\"   Found: {len(classes)} classes, \"\n",
    "              f\"{sum(len(imgs) for imgs in classes.values()):,} images\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {crop.upper()} - Path not found\")\n",
    "\n",
    "total_classes = sum(len(classes) for classes in all_classes.values())\n",
    "total_images = sum(len(imgs) for crop_classes in all_classes.values() \n",
    "                   for imgs in crop_classes.values())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üìà TOTAL: {total_classes} classes, {total_images:,} images\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: Create Dataset Class\n",
    "# ============================================\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Build unified dataset\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "class_names = []\n",
    "\n",
    "current_idx = 0\n",
    "for crop, classes in all_classes.items():\n",
    "    for class_name, paths in classes.items():\n",
    "        if len(paths) < 100:  # Skip tiny classes\n",
    "            continue\n",
    "        class_names.append(f\"{crop}___{class_name}\")\n",
    "        all_image_paths.extend(paths)\n",
    "        all_labels.extend([current_idx] * len(paths))\n",
    "        current_idx += 1\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"‚úÖ Created dataset: {num_classes} classes, {len(all_image_paths):,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 6: Data Loaders (‚ö° Optimized)\n",
    "# ============================================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create and split\n",
    "full_dataset = PlantDiseaseDataset(all_image_paths, all_labels, train_transform)\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(full_dataset, [train_size, val_size, test_size],\n",
    "                                          generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# ‚ö° Optimized loaders for Colab T4\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=4, pin_memory=True, prefetch_factor=2,\n",
    "                          persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False,\n",
    "                         num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"  Training:   {len(train_ds):,} images\")\n",
    "print(f\"  Validation: {len(val_ds):,} images\")\n",
    "print(f\"  Test:       {len(test_ds):,} images\")\n",
    "print(f\"\\n‚ö° Optimizations: batch={batch_size}, workers=4, pin_memory=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 7: Create Model (EfficientNet-B4)\n",
    "# ============================================\n",
    "model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "\n",
    "print(f\"ü§ñ Model: EfficientNet-B4\")\n",
    "print(f\"üìä Classes: {num_classes}\")\n",
    "print(f\"‚öôÔ∏è  Optimizer: AdamW (lr=0.0001)\")\n",
    "print(f\"üìÖ Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 8: Training Loop (‚ö° Mixed Precision)\n",
    "# ============================================\n",
    "import time\n",
    "\n",
    "epochs = 30\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING TRAINING (Mixed Precision + cuDNN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # ‚ö° Mixed Precision\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', \n",
    "                         'acc': f'{100.*train_correct/train_total:.1f}%'})\n",
    "    \n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'class_names': class_names,\n",
    "            'num_classes': num_classes\n",
    "        }, 'pakistan_model_best.pth')\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    eta = (epochs - epoch - 1) * elapsed / 60\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%\")\n",
    "    print(f\"   Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%, Best={best_val_acc:.2f}%\")\n",
    "    print(f\"   ‚è±Ô∏è Time: {elapsed:.0f}s | ETA: {eta:.1f} min\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 9: Test Evaluation\n",
    "# ============================================\n",
    "# Load best model\n",
    "checkpoint = torch.load('pakistan_model_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "print(\"\\nüß™ Evaluating on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_acc = 100. * test_correct / test_total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüéØ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"‚úÖ Correct: {test_correct:,} / {test_total:,}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2591512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 10: Save Model and Download\n",
    "# ============================================\n",
    "# Save metadata\n",
    "model_info = {\n",
    "    'class_names': class_names,\n",
    "    'num_classes': num_classes,\n",
    "    'test_accuracy': test_acc,\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'model_architecture': 'efficientnet_b4',\n",
    "    'crops': list(all_classes.keys()),\n",
    "    'total_images': len(all_image_paths)\n",
    "}\n",
    "\n",
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Files saved:\")\n",
    "print(\"  ‚úì pakistan_model_best.pth (model weights)\")\n",
    "print(\"  ‚úì class_names.json (class labels)\")\n",
    "print(\"  ‚úì model_info.json (metadata)\")\n",
    "\n",
    "# Download files to your computer\n",
    "print(\"\\nüì• Downloading files to your computer...\")\n",
    "from google.colab import files\n",
    "files.download('pakistan_model_best.pth')\n",
    "files.download('class_names.json')\n",
    "files.download('model_info.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 11: Plot Training History\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].axhline(y=test_acc, color='r', linestyle='--', linewidth=2, label=f'Test Acc: {test_acc:.1f}%')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Download the plot\n",
    "files.download('training_history.png')\n",
    "\n",
    "print(\"\\nüéâ DONE! All files downloaded.\")\n",
    "print(\"\\nüìÅ Copy these files to your Flask app folder:\")\n",
    "print(\"   ‚Ä¢ pakistan_model_best.pth\")\n",
    "print(\"   ‚Ä¢ class_names.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bb90f",
   "metadata": {},
   "source": [
    "## ‚úÖ Complete!\n",
    "\n",
    "### üì• Downloaded Files:\n",
    "1. `pakistan_model_best.pth` (~75 MB) - Model weights\n",
    "2. `class_names.json` - Class labels\n",
    "3. `model_info.json` - Metadata\n",
    "4. `training_history.png` - Training plot\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Copy downloaded files to `Flask Deployed App/` folder\n",
    "2. Update `app.py` to load the new model\n",
    "3. Run the Flask app!\n",
    "\n",
    "### ⁄©ÿ≥ÿßŸÜ ŸÖÿØÿØ⁄Øÿßÿ± - Helping Pakistani Farmers with AI üáµüá∞"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
